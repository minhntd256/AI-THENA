---
---

@string{aps = {American Physical Society,}}

@book{einstein1920relativity,
  title={Relativity: the Special and General Theory},
  author={Einstein, Albert},
  year={1920},
  publisher={Methuen & Co Ltd},
  html={relativity.html}
}

@book{einstein1956investigations,
  bibtex_show={true},
  title={Investigations on the Theory of the Brownian Movement},
  author={Einstein, Albert},
  year={1956},
  publisher={Courier Corporation},
  preview={brownian-motion.gif}
}

@article{einstein1950meaning,
  abbr={AJP},
  bibtex_show={true},
  title={The meaning of relativity},
  author={Einstein, Albert and Taub, AH},
  journal={American Journal of Physics},
  volume={18},
  number={6},
  pages={403--404},
  year={1950},
  publisher={American Association of Physics Teachers}
}

@article{PhysRev.47.777,
  abbr={PhysRev},
  title={Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
  author={Einstein*†, A. and Podolsky*, B. and Rosen*, N.},
  abstract={In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.},
  journal={Phys. Rev.},
  location={New Jersey},
  volume={47},
  issue={10},
  pages={777--780},
  numpages={0},
  year={1935},
  month={May},
  publisher=aps,
  doi={10.1103/PhysRev.47.777},
  url={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  html={https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
  pdf={example_pdf.pdf},
  altmetric={248277},
  dimensions={true},
  google_scholar_id={qyhmnyLat1gC},
  video={https://www.youtube-nocookie.com/embed/aqz-KE-bpKQ},
  additional_info={. *More Information* can be [found here](https://github.com/alshedivat/al-folio/)},
  annotation={* Example use of superscripts<br>† Albert Einstein},
  inspirehep_id = {3255}
}

@article{einstein1905molekularkinetischen,
  title={{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten Teilchen},
  author={Einstein, A.},
  journal={Annalen der physik},
  volume={322},
  number={8},
  pages={549--560},
  year={1905},
  publisher={Wiley Online Library}
}

@article{einstein1905movement,
  abbr={Ann. Phys.},
  title={Un the movement of small particles suspended in statiunary liquids required by the molecular-kinetic theory 0f heat},
  author={Einstein, A.},
  journal={Ann. Phys.},
  volume={17},
  pages={549--560},
  year={1905}
}

@article{einstein1905electrodynamics,
  title={On the electrodynamics of moving bodies},
  author={Einstein, A.},
  year={1905}
}

@Article{einstein1905photoelectriceffect,
  bibtex_show={true},
  abbr={Ann. Phys.},
  title="{{\"U}ber einen die Erzeugung und Verwandlung des Lichtes betreffenden heuristischen Gesichtspunkt}",
  author={Albert Einstein},
  abstract={This is the abstract text.},
  journal={Ann. Phys.},
  volume={322},
  number={6},
  pages={132--148},
  year={1905},
  doi={10.1002/andp.19053220607},
  award={Albert Einstein receveid the **Nobel Prize in Physics** 1921 *for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect*},
  award_name={Nobel Prize}
}

@book{przibram1967letters,
  bibtex_show={true},
  title={Letters on wave mechanics},
  author={Einstein, Albert and Schrödinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year={1967},
  publisher={Vision},
  preview={wave-mechanics.gif},
  abbr={Vision}
}

@Article{le2021multi,
  bibtex_show={true},
  abbr={Neurocomputing},
  title={Multi visual and textual embedding on visual question answering for blind people},
  author={Tung Le and Huy Tien Nguyen and Minh Le Nguyen},
  abstract={Highlights•Propose the delicate combination of ResNet and VGG to extract residual and global features from images.•Utilize BERT to extract the textual embedding.•Enhance the text and image understanding by stacked attention.•Succeed in both practical and scientific experiments.},
  journal={Neurocomputing},
  volume={465},
  pages={451--464},
  year={2021},
}

@Article{nguyen2022phrasetransformer,
  bibtex_show={true},
  abbr={Appl. Intell.},
  title={PhraseTransformer: an incorporation of local context information into sequence-to-sequence semantic parsing},
  author={Phuong Minh Nguyen and Tung Le and Huy Tien Nguyen and Vu Tran and Minh Le Nguyen},
  abstract={Semantic parsing is a challenging task mapping a natural language utterance to machine‑understandable information representation. We propose PhraseTransformer, which incorporates Long Short‑Term Memory into the Transformer’s self‑attention to capture local phrase context, achieving more detailed meaning representation and state‑of‑the‑art results on Geo, MSParS, and ATIS datasets, while maintaining O(1) sequential operations and demonstrating effectiveness on three translation tasks (IWSLT14 DE‑EN, IWSLT15 VI‑EN, WMT14 EN‑DE).},
  journal={Applied Intelligence},
  volume={53},
  number={12},
  pages={15889--15908},
  year={2022},
  doi={10.1007/s10489-022-04246-0}
}

@Article{dang2022subtst,
  bibtex_show={true},
  abbr={Appl. Intell.},
  title={SubTST: a consolidation of sub-word latent topics and sentence transformer in semantic representation},
  author={Binh Dang and Tung Le and Le‑Minh Nguyen},
  abstract={We propose SubTST, a novel architecture that integrates sub‑word latent topic features with Sentence‑Transformer (inspired by Sentence‑BERT and tBERT), leveraging both topic and linguistic information. Experimental and ablation studies on benchmark datasets for Semantic Textual Similarity and Semantic Similarity Detection demonstrate the strength of our approach in improving semantic representation across tasks.},
  journal={Applied Intelligence},
  volume={53},
  number={11},
  pages={13470--13487},
  year={2022},
  doi={10.1007/s10489-022-04184-x},
  selected={true},
}